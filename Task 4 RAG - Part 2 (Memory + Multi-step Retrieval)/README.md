# ğŸ§  Task 4 â€“ RAG with Memory & Multi-Turn Document QA

### ğŸ§© Overview
This task is part of the **ManuWorks AI Suite** and extends the basic RAG system (Task 3) by adding **memory and multi-turn conversational capabilities**.  
Users can ask follow-up questions over PDFs such as SOPs, machine manuals, or ISO standards, and the AI will **remember the context** across multiple turns using **LangChain ConversationBufferMemory** and **MultiQueryRetriever**.  

---

### ğŸ“‚ Folder Contents
```

Task 4 RAG - Part 2 (Memory + Multi-step Retrieval)/
â”œâ”€â”€ app.py                     # Streamlit app entry point
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ settings.yaml          # Configuration for PDF path, index path, model, etc.
â”œâ”€â”€ data/                      # Example PDFs uploaded by user
â”œâ”€â”€ index/                     # FAISS index folder
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Load YAML configuration
â”‚   â”œâ”€â”€ loader.py              # PDF loading and splitting
â”‚   â”œâ”€â”€ embedder.py            # FAISS index creation/loading
â”‚   â”œâ”€â”€ memory_qa_system.py    # Memory-augmented RAG pipeline
â”‚   â”œâ”€â”€ retriever.py           # Vectorstore -> retriever
â”‚   â””â”€â”€ utils.py               # Helper functions (e.g., print_result)
â”œâ”€â”€ vectorstore.pkl            # Cached vectorstore
â”œâ”€â”€ .env                       # Environment variables (GENAI_API_KEY)
â””â”€â”€ requirements.txt           # Required Python packages

````

---

### âš™ï¸ Tech Stack
- **Frontend:** Streamlit  
- **LLM:** Google Gemini Pro via `google-generativeai`  
- **Framework:** LangChain + `langchain_community`  
- **Vector Store:** FAISS (`faiss-cpu`)  
- **Embeddings:** `sentence-transformers` (`all-MiniLM-L6-v2`)  
- **PDF Handling:** `pypdf` via LangChain loaders  
- **Memory:** ConversationBufferMemory + MultiQueryRetriever  
- **Language:** Python 3.10+  

---

### ğŸš€ Setup & Run Instructions
1. Activate your virtual environment:
```bash
# Windows PowerShell
.venv\Scripts\activate

# Mac/Linux
source .venv/bin/activate
````

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Edit the configuration `configs/settings.yaml` if needed:

```yaml
pdf_path: "data/SOP_Hazardous_Processes.pdf"
index_path: "index/sop_faiss_index"
chunk_size: 500
chunk_overlap: 100
retrieval_k: 3
llm_model: "gemini-2.5-flash"
```

4. Run the Streamlit app:

```bash
cd "Task 4 RAG - Part 2 (Memory + Multi-step Retrieval)"
streamlit run app.py
```

5. Upload a PDF using the sidebar.
6. Ask questions in the input field. The system supports **multi-turn queries** and shows:

   * Generated answer
   * Expandable source documents
   * Conversation history

âš¡ **Note:** The first run may take longer for PDF indexing. Subsequent queries are faster.

---

### ğŸ§© Requirements (for standalone use)

If you want to run **only Task 4**:

* **Python version:** 3.10+
* **Dependencies:**

```bash
pip install streamlit langchain langchain_community faiss-cpu sentence-transformers transformers pypdf pyyaml google-generativeai tqdm
```

* **Environment Variables (.env):**

```bash
GENAI_API_KEY=your_gemini_api_key_here
```

---

### ğŸ’¬ Example Interaction

* Q1: "List the safety steps for the hydraulic press."
* Q2: "Compare these steps with the CNC lathe."

ğŸ“„ *Refer to the included PDF file for example Q&A interactions.*

---

### ğŸ“¸ Screenshots

![alt text](<Screenshot 2025-10-11 012252.png>)
![alt text](<Screenshot 2025-10-11 013842.png>)

---

### ğŸ§  What This Task Demonstrates

* âœ… Memory-augmented RAG for **multi-turn conversation**
* âœ… Context-aware answers across follow-up queries
* âœ… MultiQueryRetriever for better recall of document context
* âœ… FAISS vectorstore caching for speed
* âœ… Streamlit UI with **conversation history** and **source references**

---

### ğŸª„ Future Enhancements

* Support multiple PDFs simultaneously
* Integrate hierarchical document retrievers
* Add summarization of previous conversation context
